{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import cv2\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn import svm\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pickle\n",
    "import time\n",
    "from datasets.load import load_images\n",
    "import os\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0)  # set default size of plots\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15.0, 12.0)  # set default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "please ony change the parameters provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'cache/'      # directory to save trained model\n",
    "DATASET_DIR = 'datasets/' # directory to the dataset provided in pascal VOC format\n",
    "TRAIN_MODEL = False       # Weather or not to train a new SVM model\n",
    "                          # change to True to retrain\n",
    "DETECTIONS_DIR = 'detections' # directory to save detection .txt files\n",
    "\n",
    "SIZE = 128                # pixel size of the training window\n",
    "\n",
    "# mapping of label names to integers, required for multi label classification in sklearn\n",
    "LABELS = {\"waldo\": 0, \"wenda\": 1, \"wizard\": 2, \"negative\": 3}\n",
    "K = 5                     # K used for kmeans preprocessing\n",
    "\n",
    "\n",
    "# config for the feature descriptor (do not change)\n",
    "ORIENTATIONS_G = 9\n",
    "PIXELS_PER_CELL_G = (16,16)\n",
    "CELLS_PER_BLOCK_G = (4,4)\n",
    "ORIENTATIONS_C = 9\n",
    "PIXELS_PER_CELL_C = (16,16)\n",
    "CELLS_PER_BLOCK_C = (4,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"neg_samples_train.pkl\"), 'rb') as f:\n",
    "    neg_samples = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"waldo_train.pkl\"), 'rb') as f:\n",
    "    pos_waldo = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"wenda_train.pkl\"), 'rb') as f:\n",
    "    pos_wenda = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"wizard_train.pkl\"), 'rb') as f:\n",
    "    pos_wizard = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Descritpor\n",
    "\n",
    "For feature description we use a combination of the HOG (Historgram Orientated Gradients) Descriptor and a SIFT (Scale Invariant Feautre Transform). \n",
    "\n",
    "Both detectors generate a $3600$ dimensional feature vector and are concatenated together, creating a $7200$ dimensional feature vector in total.\n",
    "\n",
    "Below we visualize the results of the HOG descriptor (only part of the final descriptor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# viualize the descriptor once for each class\n",
    "for i, s in enumerate([pos_waldo[0], pos_wenda[0], pos_wizard[0], neg_samples[0]]):\n",
    "    feature, hog_image = hog(\n",
    "        s,\n",
    "        orientations=ORIENTATIONS_G,\n",
    "        pixels_per_cell=PIXELS_PER_CELL_G,\n",
    "        cells_per_block=CELLS_PER_BLOCK_G,\n",
    "        visualize=True,\n",
    "        multichannel=False,\n",
    "    )\n",
    "    print(\n",
    "        \"Sample image shape {}, feature vector shape {}\".format(s.shape, feature.shape)\n",
    "    )\n",
    "    plt.subplot(2, 4, 2 * i + 1)\n",
    "    plt.imshow(s)\n",
    "    plt.subplot(2, 4, 2 * i + 2)\n",
    "    plt.imshow(hog_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls into `feauture_descritptor.py` for the describe function. Which calls to `sklearn` for the HOG part of the descriptor. The SIFT-like descriptor is called HOV (Histograms over Values) is adapted from the implementaion of HOG in `sklean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_descriptor import describe\n",
    "\n",
    "def feature_descriptor(samples, parallel=False):\n",
    "    res = None\n",
    "    if parallel:\n",
    "        res = Parallel(n_jobs=cpu_count())(\n",
    "            delayed(describe)(\n",
    "                s,\n",
    "                orientations_g=ORIENTATIONS_G,\n",
    "                pixels_per_cell_g=PIXELS_PER_CELL_G,\n",
    "                cells_per_block_g=CELLS_PER_BLOCK_G,\n",
    "                orientations_c=ORIENTATIONS_C,\n",
    "                pixels_per_cell_c=PIXELS_PER_CELL_C,\n",
    "                cells_per_block_c=CELLS_PER_BLOCK_C,\n",
    "            )\n",
    "            for s in samples\n",
    "        )\n",
    "    else:\n",
    "        res = [\n",
    "            describe(\n",
    "                s,\n",
    "                orientations_g=ORIENTATIONS_G,\n",
    "                pixels_per_cell_g=PIXELS_PER_CELL_G,\n",
    "                cells_per_block_g=CELLS_PER_BLOCK_G,\n",
    "                orientations_c=ORIENTATIONS_C,\n",
    "                pixels_per_cell_c=PIXELS_PER_CELL_C,\n",
    "                cells_per_block_c=CELLS_PER_BLOCK_C,\n",
    "            )\n",
    "            for s in samples\n",
    "        ]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming all positive samples into feature space using the afore mentioned descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_wa = feature_descriptor(pos_waldo, parallel=True)\n",
    "pf_we = feature_descriptor(pos_wenda, parallel=True)\n",
    "pf_wi = feature_descriptor(pos_wizard, parallel=True)\n",
    "nf_s = feature_descriptor(neg_samples, parallel=True)\n",
    "\n",
    "X = np.concatenate((pf_wa, pf_we, pf_wi, nf_s), axis=0)\n",
    "Y = np.concatenate(\n",
    "    (\n",
    "        np.full(pf_wa.shape[0], LABELS[\"waldo\"]),\n",
    "        np.full(pf_we.shape[0], LABELS[\"wenda\"]),\n",
    "        np.full(pf_wi.shape[0], LABELS[\"wizard\"]),\n",
    "        np.full(nf_s.shape[0], LABELS[\"negative\"]),\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "x_train, y_train = X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training data shape: examples: {}, labels: {}\".format(x_train.shape, y_train.shape))\n",
    "\n",
    "pos = pf_wa.shape[0] + pf_we.shape[0] + pf_wi.shape[0]\n",
    "neg = nf_s.shape[0]\n",
    "tot = X.shape[0]\n",
    "\n",
    "print('waldo:    {} {:2.2f}%'.format(pf_wa.shape[0], pf_wa.shape[0]*100/tot))\n",
    "print('wenda:    {} {:2.2f}%'.format(pf_we.shape[0], pf_we.shape[0]*100/tot))\n",
    "print('wizard:   {} {:2.2f}%'.format(pf_wi.shape[0], pf_wi.shape[0]*100/tot))\n",
    "print('negative: {} {:2.2f}%'.format(nf_s.shape[0], nf_s.shape[0]*100/tot))\n",
    "\n",
    "\n",
    "print('positive examples:\\t{}\\t{:2.2f}% \\nnegative examples\\t{}\\t{:2.2f}%'.format(pos, pos*100/tot, neg, neg*100/tot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train Support Vector Machine Classifier (SVC)\n",
    " \n",
    " Here we train the actual SVM (Support Vector Machine) classifier form the training samples in feautre space. In Addition to the SVM itself we train a probability model on top of it, to give meaningful probability / confidence scores for each detection. This requires cross validation and cannot be done in parallel and therefore has a **large impact on training time**. \n",
    " \n",
    "As can be seen from the commented out lines, for the parameter choices of the SVM we used `sklearn`'s inbuilt hyperparamerter search function. Out of all the possible configurations tested, the ones used in line 18 are the ones providing best results.\n",
    "\n",
    "If one wants to retrain the SVM, we provide one in the cache directory. Just set `TRAIN_MODEL` in the configuration section in the beginning to true and run the cell below. Beware though, this process can take some time, as we cannot use multithreading to train this SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    #param_grid = {\n",
    "    #    'C': [1e-3, 0.01, 0.1, 10, 100, 1000],\n",
    "    #    'gamma': [1e-4, 1e-3, 0.01, 0.1, 1, 10, 100],\n",
    "    #    'kernel': ['rbf', 'linear'],\n",
    "    #}\n",
    "\n",
    "    # This uses hyperparameter search to find the best model.\n",
    "    # Therefore it can be quite slow!\n",
    "    # clf = GridSearchCV(svm.SVC(class_weight='balanced', random_state=0, decision_function_shape='ovo', probability=True),\n",
    "    #                  param_grid, refit=True, n_jobs=-1, cv=5)\n",
    "\n",
    "    clf = svm.SVC(C=10, gamma=0.01, kernel='rbf', class_weight='balanced', random_state=0, decision_function_shape='ovo', probability=True)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    #print('best parameters:')\n",
    "    #print(clf.best_params_)\n",
    "    #print(clf.best_estimator_)\n",
    "\n",
    "    # save model to disk\n",
    "    with open(os.path.join(MODEL_DIR, \"svm_classifier.pkl\"), 'wb') as f:\n",
    "        pickle.dump(clf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"svm_classifier.pkl\"), 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Classifier Evaluation\n",
    " \n",
    "We validate our training against the validation set provided in the dataset. The samples first get loaded, then transformed into feature space with the same descriptor, as the testing samples and then classified.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "with open(os.path.join(MODEL_DIR, \"waldo_test.pkl\"), 'rb') as f:\n",
    "    test_waldo = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"wenda_test.pkl\"), 'rb') as f:\n",
    "    test_wenda = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"wizard_test.pkl\"), 'rb') as f:\n",
    "    test_wizard = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"neg_samples_test.pkl\"), 'rb') as f:    \n",
    "    test_neg_samples = pickle.load(f)[:20]\n",
    "\n",
    "# transform samples into feature space\n",
    "test_waldo = feature_descriptor(test_waldo, parallel=True)\n",
    "test_wenda = feature_descriptor(test_wenda, parallel=True)\n",
    "test_wizard = feature_descriptor(test_wizard, parallel=True)\n",
    "test_neg_samples = feature_descriptor(test_neg_samples, parallel=True)\n",
    "\n",
    "x_test = np.concatenate((test_waldo, test_wenda, test_wizard, test_neg_samples), axis=0)\n",
    "y_test = np.concatenate(\n",
    "    (\n",
    "        np.full(test_waldo.shape[0], LABELS[\"waldo\"]),\n",
    "        np.full(test_wenda.shape[0], LABELS[\"wenda\"]),\n",
    "        np.full(test_wizard.shape[0], LABELS[\"wizard\"]),\n",
    "        np.full(test_neg_samples.shape[0], LABELS[\"negative\"]),\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Report\n",
    "\n",
    "The classification report gives a good overview of the classifier's performance, by computing precision, recall and $F_1$ (harmonic average of precision and recall) scores.\n",
    "Also an overall average of all these scores is given, as well as one set per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(x_test),\n",
    "                           target_names=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix gives a good viusual overview of how well the detection performed. Ideally there would only be values on the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.imshow(conf_matrix, interpolation='nearest')\n",
    "ax.set(xticks=np.arange(conf_matrix.shape[1]),\n",
    "       yticks=np.arange(conf_matrix.shape[0]),\n",
    "       xticklabels=LABELS, yticklabels=LABELS,\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "To turn our trained classifier into a true detector, we use a sliding window approach. Here many $(SIZE \\cdot SIZE)$px windows are generated and if any one of them is predicted to be one of the positive classes (`waldo`, `wenda`, `wizard`) it is added to the list of detections.\n",
    "\n",
    "In detail, we transform each window to feautre space, by first scaling it to $(SIZE \\cdot SIZE) $ px, then performing the same kmeans preprocessing step on it, as in the sample generation and the feeding that result into our descriptor, explained above. To speed this process up, we transform all windows in parallel. Still this step might take some time, depending on the available computational ressouces.\n",
    "\n",
    "For each transformed window, now in feature space, we run the classifier. If it's result is one of the positive classes, we query the associated probailiy $p$ with that class. Also we compute the $p_{raito} = \\frac{p_{max}}{p_{max-1}}$ value, which is the ratio of the two highest probabilites, giving a measue of how confident the classifier is of that class in relation to all other ones.\n",
    "\n",
    "Once all windows have been classified, we perform non-maximum suppression. For this we generate groups for all sets of overlapping positive classifications. Then choose the representative with the maximum $p$ probability for each window. Additionally we remove all windows where $p_{ratio} < 1.5$. This removes detections where the classifier is generally inconfident, where the value of $1.5$ was determined experimentally.\n",
    "\n",
    "In the last step the remaining detections after non-maximum suppression are plotted with ground truth bounding boxes if they were given to the predictor. For each prediction the following information is given:\n",
    "$ (name, p, p_{ratio}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the kmeans model from disk\n",
    "with open(os.path.join(MODEL_DIR, \"neg_samples_test.pkl\"), 'rb') as f:   \n",
    "    kmeans = pickle.load(f)\n",
    "    \n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d), dtype=np.uint8)\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i,j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "def process_img(img):\n",
    "    w,h,d = img.shape\n",
    "    labels = kmeans.predict(np.reshape(img, (w*h,d)))\n",
    "    \n",
    "    white_idx =  np.argmin([np.linalg.norm(np.array([253, 253, 253]) - c) for c in kmeans.cluster_centers_])\n",
    "    other_col_idx = np.argmin([np.linalg.norm(np.array([182,159,154]) - c) for c in kmeans.cluster_centers_])\n",
    "    labels[labels == other_col_idx] = white_idx\n",
    "    \n",
    "    c = (np.arange(0,K) * 255 / (K-1)).astype(np.uint8)[np.newaxis].T\n",
    "    cluster_centers = np.hstack([c,c,c])\n",
    "    return recreate_image(cluster_centers, labels, w,h)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is adapted from https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n",
    "def bbx_iou(bbx1, bbx2):\n",
    "    assert bbx1['x1'] < bbx1['x2']\n",
    "    assert bbx1['y1'] < bbx1['y2']\n",
    "    assert bbx2['x1'] < bbx2['x2']\n",
    "    assert bbx2['y1'] < bbx2['y2']\n",
    "    \n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bbx1['x1'], bbx2['x1'])\n",
    "    y_top = max(bbx1['y1'], bbx2['y1'])\n",
    "    x_right = min(bbx1['x2'], bbx2['x2'])\n",
    "    y_bottom = min(bbx1['y2'], bbx2['y2'])\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # compute the area of both AABBs\n",
    "    bbx1_area = (bbx1['x2'] - bbx1['x1']) * (bbx1['y2'] - bbx1['y1'])\n",
    "    bbx2_area = (bbx2['x2'] - bbx2['x1']) * (bbx2['y2'] - bbx2['y1'])\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bbx1_area + bbx2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(detections, threshold=0.0):\n",
    "    if len(detections) == 0: return detections\n",
    "    \n",
    "    groups = [[detections[0]]]\n",
    "    # group detections if overlap is greather than threshold\n",
    "    for d in detections:\n",
    "        for idx, g in enumerate(groups):\n",
    "            if np.max([bbx_iou(d['w'], bbx['w']) for bbx in g]) > threshold:\n",
    "                groups[idx].append(d)\n",
    "                break\n",
    "        else:\n",
    "            groups.append([d])\n",
    "        \n",
    "    detections = []\n",
    "    for g in groups:\n",
    "        detections.append(g[np.argmax(x['p'] for x in g)])\n",
    "    # delte detection if p_ratio is less than 1.5\n",
    "    detections = [d for d in detections if d['p_ratio'] > 1.5]\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(ax, text, box, col='r'):\n",
    "    r = patches.Rectangle((box['x1'], box['y1']), \n",
    "                          box['x2'] - box['x1'],\n",
    "                          box['y2'] - box['y1'],\n",
    "                          linewidth=3,\n",
    "                          edgecolor=col,\n",
    "                          facecolor='none')\n",
    "    ax.add_patch(r)\n",
    "    ax.text(box['x1'], box['y1'], text,\n",
    "            bbox={'facecolor': col, 'linewidth': 0})\n",
    "\n",
    "def draw_annotation(ax, classes):\n",
    "    for idx, cl in enumerate(classes):\n",
    "        col = 'g'\n",
    "        for box in cl['bounds']:\n",
    "            draw_bbox(ax, cl['name'], box, col)\n",
    "\n",
    "def dict_invert(key, d):\n",
    "    return set(k for k, v in d.items() if v == key).pop()\n",
    "\n",
    "def variance(img, w=None, size=250):\n",
    "    if w is not None:\n",
    "        img = img[w['y1']:w['y2'], w['x1']:w['x2']]\n",
    "\n",
    "    r = np.random.choice(img[:,:,0].ravel(), size)\n",
    "    g = np.random.choice(img[:,:,1].ravel(), size)\n",
    "    b = np.random.choice(img[:,:,2].ravel(), size)\n",
    "    return np.array([np.var(r), np.var(g), np.var(b)])\n",
    "\n",
    "def _transform_feature(img, w, var):\n",
    "    #if np.max(variance(img, w)) < var:\n",
    "    #    return ({}, None)\n",
    "    window = cv2.resize(img[w['y1']:w['y2'], w['x1']:w['x2']], dsize=(128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "    return (w, feature_descriptor([process_img(window)]))\n",
    "\n",
    "def _predict(w, feat_vect):\n",
    "    if feat_vect is None: return None\n",
    "    \n",
    "    label = clf.predict(feat_vect)[0]\n",
    "    \n",
    "    \n",
    "    if LABELS['negative'] != label:\n",
    "        proba = clf.predict_proba(feat_vect)[0]\n",
    "        p_ratio = proba[label] / np.max(proba[proba != proba[label]])\n",
    "        \n",
    "        d = {'w':w, 'class':dict_invert(label, LABELS), 'p':proba[label], 'p_ratio': p_ratio}\n",
    "        print(d)\n",
    "        return d\n",
    "    return {}\n",
    "\n",
    "def detect_img(img, ground_truth=None):\n",
    "    img_y, img_x = img.shape[:2]\n",
    "    size = 128\n",
    "                  \n",
    "    var = np.mean(variance(img, size=1000)) * 2/3\n",
    "    print('performing detection on image of size {} with window size {}'.format(img.shape, size))\n",
    "        \n",
    "    with mp.Pool(mp.cpu_count()) as p:\n",
    "        args = []\n",
    "        for scale in [1, 2, 4, 6]:\n",
    "            window_size = int(size * scale)\n",
    "            for y in range(0, img_y - window_size, window_size // 4):\n",
    "                for x in range(0, img_x - window_size, window_size // 4):\n",
    "                    args.append((img, \n",
    "                                 {'y1':y, 'y2':y + window_size, \n",
    "                                  'x1':x, 'x2':x + window_size},\n",
    "                                 var))\n",
    "\n",
    "        print(\"transforming image windows into feature space\")\n",
    "\n",
    "        feature_vectors = p.starmap(_transform_feature, args)\n",
    "\n",
    "        print('... done')\n",
    "        print(\"detecting classes for windows\")\n",
    "        print('detections:')\n",
    "        \n",
    "        detections = p.starmap(_predict, feature_vectors)\n",
    "        \n",
    "        \n",
    "        omitted = 0\n",
    "        detections_ = []\n",
    "        for d in detections:\n",
    "            if d: detections_.append(d)\n",
    "            elif d is None: omitted += 1\n",
    "        detections = detections_   \n",
    "\n",
    "    \n",
    "    print('omitted {:2f}% of windows'.format(omitted / len(feature_vectors)*100))\n",
    "    print('evaluated {} windows'.format(len(feature_vectors)))\n",
    "    \n",
    "    detections = non_max_suppression(detections)\n",
    "    print('num classes after non_max_supp: {}'.format(len(detections)))\n",
    "    \n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.imshow(img)\n",
    "    # draw gound truth\n",
    "    #draw_annotation(ax, sample['classes'])\n",
    "\n",
    "    # draw detections\n",
    "    for d in detections:\n",
    "        draw_bbox(ax, '{} $({:2.1f},{:2.1f})$'.format(d['class'], d['p'], d['p_ratio']), d['w'])\n",
    "    if ground_truth is not None:\n",
    "        draw_annotation(ax, ground_truth)\n",
    "        \n",
    "    plt.show()\n",
    "    return np.array(detections)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(img, min_size=3000, max_size=4096, ground_truth_boxes=None):\n",
    "    h,w = img.shape[:2]\n",
    "    if max(h,w) > max_size:\n",
    "        if w > h:\n",
    "            dsize = (max_size, int(h * max_size / w))\n",
    "            s = max_size/w\n",
    "        else:\n",
    "            dsize = (int(w * max_size / h), max_size)\n",
    "            s = max_size/h\n",
    "    elif min(h,w) < min_size:\n",
    "        if w > h:\n",
    "            dsize = (int(w * min_size / h), min_size)\n",
    "            s = min_size/h\n",
    "        else:\n",
    "            dsize = (min_size, int(h * min_size / w))\n",
    "            s = min_size/w\n",
    "\n",
    "    img = cv2.resize(img, dsize=dsize, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    if ground_truth_boxes:\n",
    "        gt = []\n",
    "        for cl in ground_truth_boxes:\n",
    "            gt.append({\n",
    "                'name': cl['name'],\n",
    "                'bounds': [{'x1': int(b['x1']*s), 'y1': int(b['y1']*s),\n",
    "                            'x2': int(b['x2']*s), 'y2': int(b['y2']*s)} for b in cl['bounds']]\n",
    "            })\n",
    "        return img, gt, s\n",
    "    else:\n",
    "        return img, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of validation images\n",
    "\n",
    "As an example we run the classifier on the validation training set. However all of the images in the dataset have varying sizes. Assuming all of them are scans from the same book (i.e. same paper size) we can confidently rescale all images to the same size (while preserving asprect ratios), to keep a consistent relation of pixel sizes given to the detector and real world size of the printed characters. \n",
    "\n",
    "For a detailed description of how these values we determined form the dataset, see `dataset_analytics.py` or the corresponding section in the report.\n",
    "\n",
    "As we use a scaled version of the images in the detector, we also scale ground truth bounding boxes and detections accordingly to make the transformation transparent to the end user.\n",
    "\n",
    "Lastly we save the detection results (`waldo.txt`, `wenda.txt` and `wizard.txt`) in the required format to `DETECTIONS_DIR`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = load_images('val', DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final detections output\n",
    "output = {\n",
    "    'waldo': [],\n",
    "    'wenda': [],\n",
    "    'wizard': []\n",
    "}\n",
    "\n",
    "for sample in samples:      \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    img, gt, s = rescale(sample['img'], ground_truth_boxes=sample['classes'])\n",
    "    detections = detect_img(img, ground_truth=gt)\n",
    "    \n",
    "    print('elapsed time {} s'.format(time.perf_counter() - start))\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    # append detections to list\n",
    "    for d in detections:\n",
    "        output[d['class']].append('{} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}'.format(sample['filename'].strip('.jpg'), \n",
    "                                                             d['p'], \n",
    "                                                             d['w']['x1']/s, \n",
    "                                                             d['w']['y1']/s, \n",
    "                                                             d['w']['x2']/s, \n",
    "                                                             d['w']['y2']/s))\n",
    "\n",
    "# write detections to file\n",
    "for name, d in output.items():\n",
    "    with open(os.path.join(DETECTIONS_DIR, '/{}.txt'.format(name)), 'w') as f:\n",
    "        f.write('\\n'.join(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP Score\n",
    "Here we display the mAP score for our detetions, as well as the one reqired in baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from voc_eval import voc_eval\n",
    "overlap = 0.1\n",
    "print('baseline results:')\n",
    "for name, _ in output.items():\n",
    "    rec, prec, ap = voc_eval('baseline/{}.txt'.format(name),\n",
    "                             os.path.join(DATASET_DIR, 'Annotations/{}.xml'.format(name)),\n",
    "                             os.path.join(DATASET_DIR,'ImageSets/val.txt'), \n",
    "                             name, \n",
    "                             MODEL_DIR, \n",
    "                             ovthresh=overlap)\n",
    "    print('{:>10}  mAP: {:.2f}'.format(name, ap))\n",
    "    \n",
    "print('detection results:')\n",
    "for name, _ in output.items():\n",
    "    rec, prec, ap = voc_eval(os.path.join(DETECTIONS_DIR, '{}.txt'.format(name)), \n",
    "                             os.path.join(DATASET_DIR, 'Annotations/{}.xml'.format(name)),\n",
    "                             os.path.join(DATASET_DIR,'ImageSets/val.txt'), \n",
    "                             name, \n",
    "                             MODEL_DIR, \n",
    "                             ovthresh=overlap)\n",
    "    print('{:>10}  mAP: {:.2f}'.format(name, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of new images\n",
    "\n",
    "To classify a new image, change the variables below to point to the image path and give a valid results path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMG_PATH = 'datasets/JPEGImages/000.jpg'\n",
    "DETECTIONS_DIR = '.'\n",
    "\n",
    "img = cv2.cvtColor(cv2.imread(IMG_PATH), cv2.COLOR_BGR2RGB)\n",
    "# final detections output\n",
    "output = {\n",
    "    'waldo': [],\n",
    "    'wenda': [],\n",
    "    'wizard': []\n",
    "}\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "img_rescaled, s = rescale(img)\n",
    "detections = detect_img(img_rescaled)\n",
    "\n",
    "print('elapsed time {} s'.format(time.perf_counter() - start))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "# append detections to list\n",
    "for d in detections:\n",
    "    _, ext = os.path.splitext(IMG_PATH)\n",
    "    img_name = os.path.basename(IMG_PATH).strip(ext)\n",
    "\n",
    "    output[d['class']].append('{} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}'.format(img_name, \n",
    "                                                         d['p'], \n",
    "                                                         d['w']['x1']/s, \n",
    "                                                         d['w']['y1']/s, \n",
    "                                                         d['w']['x2']/s, \n",
    "                                                         d['w']['y2']/s))\n",
    "\n",
    "# write detections to file\n",
    "for name, d in output.items():\n",
    "    with open(os.path.join(DETECTIONS_DIR, '/{}.txt'.format(name)), 'w') as f:\n",
    "        f.write('\\n'.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:cvpr] *",
   "language": "python",
   "name": "conda-env-cvpr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
